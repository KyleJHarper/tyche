Contains a simple note for each tagged release.  We will try to follow SEMVER.  Be aware, all 0.x.y releases are developmental and are NOT guaranteed compatible according to SEMVER.  Major version 0 should go away after major release 1... as will this notice probably.

0.0.10 (2016-03-27)
- Switched from a raw and comp list to a single list, allowing the comp_length to determine if a buffer is compressed or not.  The caller is notified of this by having a buffer__copy() made, decompresed, and is_ephemeral sent back.
- Lowered the restoration threshold (magic number) to 6 which seemed to be a better case scenario.  In truth, this needs better research and testing.
- Created a buffer__block/unblock() function-set using 2 conditions which allows anyone to safely block and edit a buffer.  This, combined with the use of a single list, means restoration and compression (sweeping) is still read-safe!  Only the removal of buffers (which only happens in to comp victims) means we get MUCH more reader time for searching.  CPU recruitment and scaling is much higher.
- Did a huuuugge amount of profiling and debugging of performance.  The hottest spot (98%+) is down to a single line in list__search() on the Skiplist scanning, which is perfect.
- Due to needing a second condition variable, buffer overhead is up to approximately 184 bytes (x86-64).  FYI.

0.0.9 (2016-03-08)
- Short release to address a theory I had about the fairness of restoration.  See points below.
- Changed list__search to only call list__restore when a RESTORATION_THRESHOLD (32) is met.  This is a magic number just like popularity is, so it may need tuning later.  If the threshold isn't met, we simply make a copy of the buffer and set its is_ephemeral flag to true.
- Changed list__restore to leave popularity alone.  Upon promotion to the raw list, it should come in with an accurate representation of the popularity it received while victimized (compressed).
- Updated all tests to support is_ephemeral checks where necessary.
- Re-ran valgrind to ensure no new leaks.  Still <1MB regardless of test duration.

0.0.8 (2016-03-06)
- Removed the locker_pool and all lock__* functions (the whole .h and .c files actually).  I like the idea for memory saving when buffers are smaller (1k or less) but the truth is, with regard to time-memory trade-offs, I can't justify the additional complexity.
- Removed the -l option now that the lock subsystem is permanently 1:1.
- Fixed another bug in tests__synchronized_readwrite.  list__destroy was being called and shouldn't have been.
- Determined that, for version 0.0.7, performance can be improved up to ~20% based on workload by using jemalloc vs malloc.  However, I'm not making a formal switch at this time since this can be changed with LD_* stuff later if needed.
- Massive improvments to list__sweep() by batching and parallizing compression.  Also used federation (see below).
- Added an option to orphan a buffer when list__remove() is called.  This federation means list__sweep() and list__restore() can move buffers between lists by simple pointer changes; no more buffer__copy() or buffer__destroy().  Massive improvement!

0.0.7 (2016-02-06)
- Switched from an array of buffers to a circular linked list.  This allows insertion and deletion to happen without growing or shrinking the entire list, nor keeping a "free" list.  See 'skiplist' for ordering/searching.
- A skiplist implementation with a traditional coin-toss (50/50) was implemented to replace the binary search we lost when we switched off of an array of buffers.
- All tests were completely overhauled to support the new list structure and skiplist indexing system.  Additional -X option sets were created to support better edge case testing.
- The manager and worker structs and functions for basic working and tracking now exist.  This will be the foothold for ACCRS logic to stem from.
- Status output was added, if desired.
- Destruction functions (*__destroy()) were created to help prevent memory leakes.  A future optimization branch will be worked on that will help eliminate even more un-free()'d memory... but for now I fixed the biggest offenders.
- The sizes binary (program) was updated to help understand per-object space requirements on each system.
- A race condition with list__restore() was causing thread 2 (the loser of the race) to cause a segmentation violation (SIGSEGV).  This was fixed.
- The list__acquire_write_lock not only flushes buffers, but the list__update_ref() function aborts early if the current thread is the owner of the write lock.  This helps avoid self-induced deadlocks within a thread and useless broadcasting.

0.0.6 (2015-12-12)
- Got the basics of buffer management done.  We can now read buffers from disk and offload them like we want.  Hooray.
- Performance tests outlined major performance problems in list management.  Will be fixed in 0.0.7 most likely.

0.0.5 (2015-12-08)
- Added a pthread_self() check to list read locks.  If we hold the write lock, skip it.  Not used yet, but frees us up to use read-only functions while inside of a write operation stack without deadlocking itself.
- Fixed a synchronization issue and reconfirmed performance is good.
- Added a new option (-X) for extended test options.
- Added a new option (-l) to control the buffer:mutex ratio to balance contention with memory sharing.
- Added the ability to run internal tests by specifying the -t option.
- Modified options to be a separate struct and much easier to share among functions in the program.

0.0.4 (2015-11-12)
- Lists now support migrating buffers between them.  Both raw -> comp and backward.
- When a raw list is out of room, it automatically tries to free up space.
- When a compressed list is out of room, it pops the least popular (oldest) generation of compressed buffers.
- When a buffer is missing from the raw list, but found in compressed list, it is restored.  If the restoration requires the raw list be swept to make room, this is supported.
- Switched list lock acquisition and relinquishment to support depth to avoid deadlocks from a single thread, since the goal of locking is always to isolate list operations to a single thread anyway.

0.0.3 (2015-11-04)
- Compression is now functional.
- Updated README and TODO.
- Few touch ups for the upcoming v0.0.4 list management stuff.
- Created the VERSIONS file (this document).

0.0.2 (2015-08-26)
- New locking subsystem completely overhauled the list add/remove functionality.  MUCH faster by means of reference counters, pinning, and a shared lock per-list rather than global locks for everything.

0.0.1 (2015-07-22)
- Basics of adding/removing buffers from a list working.

